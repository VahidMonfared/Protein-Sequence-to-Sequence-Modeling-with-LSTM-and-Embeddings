{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JunnASBjPKd",
        "outputId": "b463e50e-82e3-483e-cb67-16ae31d024b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 228ms/step - accuracy: 0.0181 - loss: 8.1502\n",
            "Epoch 2/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 226ms/step - accuracy: 0.0243 - loss: 6.4369\n",
            "Epoch 3/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 229ms/step - accuracy: 0.0696 - loss: 5.8510\n",
            "Epoch 4/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 239ms/step - accuracy: 0.1791 - loss: 5.2944\n",
            "Epoch 5/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 223ms/step - accuracy: 0.2882 - loss: 4.8021\n",
            "Epoch 6/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 229ms/step - accuracy: 0.4077 - loss: 4.3337\n",
            "Epoch 7/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 229ms/step - accuracy: 0.4896 - loss: 3.9410\n",
            "Epoch 8/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 231ms/step - accuracy: 0.5271 - loss: 3.6441\n",
            "Epoch 9/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 241ms/step - accuracy: 0.5922 - loss: 3.3903\n",
            "Epoch 10/10\n",
            "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 247ms/step - accuracy: 0.6055 - loss: 3.1787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step\n",
            "Predicted Features: [array(['P62314'], dtype=object), array(['MAADGYLPDWLEDTLSEGIRQWWKLKPGPPPPKPAERHKDDSRGLVLPGYKYLGPFNGLDKGEPVNEADAAALEHDKAYDRQLDSGDNPYLKYNHADAEFQERLKEDTSFGGNLGRAVFQAKKRVLEPLGLVEEPVKTAPGKKRPVEHSPVEPDSSSGTGKAGQQPARKRLNFGQTGDADSVPDPQPLGQPPAAPSGLGTNTMATGSGAPMADNNEGADGVGNSSGNWHCDSTWMGDRVITTSTRTWALPTYNNHLYKQISSQSGASNDNHYFGYSTPWGYFDFNRFHCHFSPRDWQRLINNNWGFRPKRLNFKLFNIQVKEVTQNDGTTTIANNLTSTVQVFTDSEYQLPYVLGSAHQGCLPPFPADVFMVPQYGYLTLNNGSQAVGRSSFYCLEYFPSQMLRTGNNFTFSYTFEDVPFHSSYAHSQSLDRLMNPLIDQYLYYLSRTNTPSGTTTQSRLQFSQAGASDIRDQSRNWLPGPCYRQQRVSKTSADNNNSEYSWTGATKYHLNGRDSLVNPGPAMASHKDDEEKFFPQSGVLIFGKQGSEKTNVDIEKVMITDEEEIRTTNPVATEQYGSVSTNLQRGNRQAATADVNTQGVLPGMVWQDRDVYLQGPIWAKIPHTDGHFHPSPLMGGFGLKHPPPQILIKNTPVPANPSTTFSAAKFASFITQYSTGQVSVEIEWELQKENSKRWNPEIQYTSNYNKSVNVDFTVDTNGVYSEPRPIGTRYLTRNL'],\n",
            "      dtype=object), array(['ANQAFKLTS'], dtype=object)]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/dataset.csv')\n",
        "\n",
        "# Features and target\n",
        "categorical_features = [\"parent_protein_id\", \"protein_seq\", \"peptide_seq\"]\n",
        "numerical_features = [\n",
        "    \"start_position\",\n",
        "    \"end_position\",\n",
        "    \"chou_fasman\",\n",
        "    \"emini\",\n",
        "    \"kolaskar_tongaonkar\",\n",
        "    \"parker\",\n",
        "    \"isoelectric_point\",\n",
        "    \"aromaticity\",\n",
        "    \"hydrophobicity\",\n",
        "    \"stability\"\n",
        "]\n",
        "target_feature = \"target\"\n",
        "\n",
        "# Encode categorical features\n",
        "label_encoders = {}\n",
        "for feature in categorical_features:\n",
        "    le = LabelEncoder()\n",
        "    data[feature] = le.fit_transform(data[feature])\n",
        "    label_encoders[feature] = le\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = MinMaxScaler()\n",
        "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
        "\n",
        "# Prepare input and target\n",
        "X_categorical = data[categorical_features].values\n",
        "X_numerical = data[numerical_features].values\n",
        "y = data[target_feature].values\n",
        "\n",
        "# Define model parameters\n",
        "sequence_max_len = X_categorical.shape[1]  # Max length of categorical features\n",
        "num_features = X_numerical.shape[1]  # Number of numerical features\n",
        "vocab_size = max([data[feature].max() for feature in categorical_features]) + 1  # Vocabulary size for sequences\n",
        "latent_dim = 64  # Latent dimension for LSTM\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs_numeric = Input(shape=(num_features,), name=\"encoder_numeric_inputs\")\n",
        "encoder_inputs_seq = Input(shape=(sequence_max_len,), name=\"encoder_sequence_inputs\")\n",
        "encoder_embedding = Embedding(vocab_size, latent_dim, name=\"encoder_embedding\")(encoder_inputs_seq)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True, name=\"encoder_lstm\")\n",
        "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(sequence_max_len,), name=\"decoder_inputs\")\n",
        "decoder_embedding = Embedding(vocab_size, latent_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "decoder_dense = Dense(vocab_size, activation=\"softmax\", name=\"decoder_dense\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Seq2Seq Model\n",
        "seq2seq_model = Model([encoder_inputs_numeric, encoder_inputs_seq, decoder_inputs], decoder_outputs, name=\"seq2seq_model\")\n",
        "seq2seq_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Data Preparation for Training\n",
        "decoder_input_data = np.zeros_like(X_categorical)  # Use shifted sequences as decoder inputs\n",
        "X_numeric_train = X_numerical\n",
        "X_seq_train = X_categorical\n",
        "y_train = np.expand_dims(X_categorical, axis=-1)  # Target output as sequence\n",
        "\n",
        "# Train the Model\n",
        "seq2seq_model.fit([X_numeric_train, X_seq_train, decoder_input_data], y_train, batch_size=32, epochs=10)\n",
        "\n",
        "# Reverse Prediction\n",
        "def reverse_predict(target, stability, emini, isoelectric_point):\n",
        "    # Prepare inputs\n",
        "    sample_numeric = scaler.transform([[stability, emini, 0, 0, 0, isoelectric_point, 0, 0, 0, 0]])\n",
        "    sample_seq = np.zeros((1, sequence_max_len))  # Empty sequence as input for decoder\n",
        "    prediction = seq2seq_model.predict([sample_numeric, sample_seq, sample_seq])\n",
        "    # Decode categorical outputs\n",
        "    decoded_output = [label_encoders[feature].inverse_transform([np.argmax(pred)]) for feature, pred in zip(categorical_features, prediction[0])]\n",
        "    return decoded_output\n",
        "\n",
        "# Example: Predict features for target=1, stability=8.9, emini=0.16, isoelectric_point=6.6\n",
        "result = reverse_predict(1, 8.9, 0.16, 6.6)\n",
        "print(\"Predicted Features:\", result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
